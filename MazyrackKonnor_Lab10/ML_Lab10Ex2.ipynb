{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekPHOtsNKj35",
        "outputId": "de840aea-858c-415b-a927-ef7c0ac2b3f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data: [[15.26   14.84    0.871  ...  2.221   5.22    1.    ]\n",
            " [14.88   14.57    0.8811 ...  1.018   4.956   1.    ]\n",
            " [14.29   14.09    0.905  ...  2.699   4.825   1.    ]\n",
            " ...\n",
            " [13.2    13.66    0.8883 ...  8.315   5.056   3.    ]\n",
            " [11.84   13.21    0.8521 ...  3.598   5.044   3.    ]\n",
            " [12.3    13.34    0.8684 ...  5.637   5.063   3.    ]]\n",
            "[[15.26   14.84    0.871  ...  3.312   2.221   5.22  ]\n",
            " [14.88   14.57    0.8811 ...  3.333   1.018   4.956 ]\n",
            " [14.29   14.09    0.905  ...  3.337   2.699   4.825 ]\n",
            " ...\n",
            " [13.2    13.66    0.8883 ...  3.232   8.315   5.056 ]\n",
            " [11.84   13.21    0.8521 ...  2.836   3.598   5.044 ]\n",
            " [12.3    13.34    0.8684 ...  2.974   5.637   5.063 ]]\n",
            "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 3. 3. 3. 3.\n",
            " 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.\n",
            " 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.\n",
            " 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]\n",
            "Iteration 1, loss = 1.18119623\n",
            "Iteration 2, loss = 1.07835467\n",
            "Iteration 3, loss = 0.98722689\n",
            "Iteration 4, loss = 0.89241184\n",
            "Iteration 5, loss = 0.79308732\n",
            "Iteration 6, loss = 0.69583924\n",
            "Iteration 7, loss = 0.60631992\n",
            "Iteration 8, loss = 0.52792233\n",
            "Iteration 9, loss = 0.46118111\n",
            "Iteration 10, loss = 0.40520975\n",
            "Iteration 11, loss = 0.35910840\n",
            "Iteration 12, loss = 0.32066952\n",
            "Iteration 13, loss = 0.28901851\n",
            "Iteration 14, loss = 0.26334490\n",
            "Iteration 15, loss = 0.24050053\n",
            "Iteration 16, loss = 0.22363033\n",
            "Iteration 17, loss = 0.20830399\n",
            "Iteration 18, loss = 0.19492449\n",
            "Iteration 19, loss = 0.18502650\n",
            "Iteration 20, loss = 0.17830658\n",
            "Iteration 21, loss = 0.17054918\n",
            "Iteration 22, loss = 0.16582690\n",
            "Iteration 23, loss = 0.16004691\n",
            "Iteration 24, loss = 0.15448989\n",
            "Iteration 25, loss = 0.14984682\n",
            "Iteration 26, loss = 0.14665723\n",
            "Iteration 27, loss = 0.14198439\n",
            "Iteration 28, loss = 0.13728490\n",
            "Iteration 29, loss = 0.13697580\n",
            "Iteration 30, loss = 0.13157098\n",
            "Iteration 31, loss = 0.12786562\n",
            "Iteration 32, loss = 0.12750351\n",
            "Iteration 33, loss = 0.12183765\n",
            "Iteration 34, loss = 0.11983380\n",
            "Iteration 35, loss = 0.11894318\n",
            "Iteration 36, loss = 0.11464252\n",
            "Iteration 37, loss = 0.11193480\n",
            "Iteration 38, loss = 0.10894369\n",
            "Iteration 39, loss = 0.10819601\n",
            "Iteration 40, loss = 0.10500882\n",
            "Iteration 41, loss = 0.10173193\n",
            "Iteration 42, loss = 0.10144480\n",
            "Iteration 43, loss = 0.10023674\n",
            "Iteration 44, loss = 0.09568632\n",
            "Iteration 45, loss = 0.09394230\n",
            "Iteration 46, loss = 0.09205115\n",
            "Iteration 47, loss = 0.08911693\n",
            "Iteration 48, loss = 0.08810152\n",
            "Iteration 49, loss = 0.08925798\n",
            "Iteration 50, loss = 0.08435638\n",
            "Iteration 51, loss = 0.08371578\n",
            "Iteration 52, loss = 0.08001850\n",
            "Iteration 53, loss = 0.07881870\n",
            "Iteration 54, loss = 0.07892917\n",
            "Iteration 55, loss = 0.07404279\n",
            "Iteration 56, loss = 0.07370435\n",
            "Iteration 57, loss = 0.07253783\n",
            "Iteration 58, loss = 0.06918901\n",
            "Iteration 59, loss = 0.06779617\n",
            "Iteration 60, loss = 0.06714522\n",
            "Iteration 61, loss = 0.06390167\n",
            "Iteration 62, loss = 0.06478020\n",
            "Iteration 63, loss = 0.06123835\n",
            "Iteration 64, loss = 0.06178809\n",
            "Iteration 65, loss = 0.05931875\n",
            "Iteration 66, loss = 0.05903751\n",
            "Iteration 67, loss = 0.05820466\n",
            "Iteration 68, loss = 0.05491390\n",
            "Iteration 69, loss = 0.05395103\n",
            "Iteration 70, loss = 0.05370474\n",
            "Iteration 71, loss = 0.05172813\n",
            "Iteration 72, loss = 0.04967212\n",
            "Iteration 73, loss = 0.04905456\n",
            "Iteration 74, loss = 0.04749513\n",
            "Iteration 75, loss = 0.04709852\n",
            "Iteration 76, loss = 0.04555828\n",
            "Iteration 77, loss = 0.04487202\n",
            "Iteration 78, loss = 0.04456736\n",
            "Iteration 79, loss = 0.04208739\n",
            "Iteration 80, loss = 0.04157012\n",
            "Iteration 81, loss = 0.04241353\n",
            "Iteration 82, loss = 0.03954424\n",
            "Iteration 83, loss = 0.03973877\n",
            "Iteration 84, loss = 0.03805868\n",
            "Iteration 85, loss = 0.03674810\n",
            "Iteration 86, loss = 0.03608185\n",
            "Iteration 87, loss = 0.03521171\n",
            "Iteration 88, loss = 0.03561205\n",
            "Iteration 89, loss = 0.03428239\n",
            "Iteration 90, loss = 0.03356107\n",
            "Iteration 91, loss = 0.03264790\n",
            "Iteration 92, loss = 0.03201623\n",
            "Iteration 93, loss = 0.03139424\n",
            "Iteration 94, loss = 0.03057094\n",
            "Iteration 95, loss = 0.02936853\n",
            "Iteration 96, loss = 0.03110497\n",
            "Iteration 97, loss = 0.02767059\n",
            "Iteration 98, loss = 0.02903573\n",
            "Iteration 99, loss = 0.02711567\n",
            "Iteration 100, loss = 0.02853856\n",
            "Iteration 101, loss = 0.02721122\n",
            "Iteration 102, loss = 0.02632561\n",
            "Iteration 103, loss = 0.02522513\n",
            "Iteration 104, loss = 0.02434170\n",
            "Iteration 105, loss = 0.02374091\n",
            "Iteration 106, loss = 0.02421345\n",
            "Iteration 107, loss = 0.02345391\n",
            "Iteration 108, loss = 0.02287302\n",
            "Iteration 109, loss = 0.02294775\n",
            "Iteration 110, loss = 0.02239278\n",
            "Iteration 111, loss = 0.02100479\n",
            "Iteration 112, loss = 0.02119890\n",
            "Iteration 113, loss = 0.02055585\n",
            "Iteration 114, loss = 0.02055443\n",
            "Iteration 115, loss = 0.01989138\n",
            "Iteration 116, loss = 0.01966406\n",
            "Iteration 117, loss = 0.01911940\n",
            "Iteration 118, loss = 0.01895875\n",
            "Iteration 119, loss = 0.01861296\n",
            "Iteration 120, loss = 0.02054344\n",
            "Iteration 121, loss = 0.01982815\n",
            "Iteration 122, loss = 0.01853855\n",
            "Iteration 123, loss = 0.01774448\n",
            "Iteration 124, loss = 0.01731437\n",
            "Iteration 125, loss = 0.01692627\n",
            "Iteration 126, loss = 0.01718734\n",
            "Iteration 127, loss = 0.01685288\n",
            "Iteration 128, loss = 0.01744287\n",
            "Iteration 129, loss = 0.01591733\n",
            "Iteration 130, loss = 0.01508501\n",
            "Iteration 131, loss = 0.01611010\n",
            "Iteration 132, loss = 0.01604218\n",
            "Iteration 133, loss = 0.01448428\n",
            "Iteration 134, loss = 0.01437291\n",
            "Iteration 135, loss = 0.01483005\n",
            "Iteration 136, loss = 0.01427590\n",
            "Iteration 137, loss = 0.01360984\n",
            "Iteration 138, loss = 0.01359997\n",
            "Iteration 139, loss = 0.01321418\n",
            "Iteration 140, loss = 0.01338479\n",
            "Iteration 141, loss = 0.01270973\n",
            "Iteration 142, loss = 0.01280720\n",
            "Iteration 143, loss = 0.01197617\n",
            "Iteration 144, loss = 0.01190266\n",
            "Iteration 145, loss = 0.01213720\n",
            "Iteration 146, loss = 0.01155720\n",
            "Iteration 147, loss = 0.01268383\n",
            "Iteration 148, loss = 0.01092805\n",
            "Iteration 149, loss = 0.01131701\n",
            "Iteration 150, loss = 0.01078597\n",
            "Iteration 151, loss = 0.01061992\n",
            "Iteration 152, loss = 0.01068902\n",
            "Iteration 153, loss = 0.01206692\n",
            "Iteration 154, loss = 0.01041272\n",
            "Iteration 155, loss = 0.01020927\n",
            "Iteration 156, loss = 0.00975413\n",
            "Iteration 157, loss = 0.00959454\n",
            "Iteration 158, loss = 0.00958135\n",
            "Iteration 159, loss = 0.00946163\n",
            "Iteration 160, loss = 0.00966629\n",
            "Iteration 161, loss = 0.00882979\n",
            "Iteration 162, loss = 0.00899483\n",
            "Iteration 163, loss = 0.00875196\n",
            "Iteration 164, loss = 0.00859847\n",
            "Iteration 165, loss = 0.00864648\n",
            "Iteration 166, loss = 0.00909484\n",
            "Iteration 167, loss = 0.00850733\n",
            "Iteration 168, loss = 0.00818027\n",
            "Iteration 169, loss = 0.00840071\n",
            "Iteration 170, loss = 0.00812691\n",
            "Iteration 171, loss = 0.00798824\n",
            "Iteration 172, loss = 0.00762675\n",
            "Iteration 173, loss = 0.00757031\n",
            "Iteration 174, loss = 0.00772265\n",
            "Iteration 175, loss = 0.00738862\n",
            "Iteration 176, loss = 0.00711404\n",
            "Iteration 177, loss = 0.00715266\n",
            "Iteration 178, loss = 0.00682236\n",
            "Iteration 179, loss = 0.00684320\n",
            "Iteration 180, loss = 0.00664282\n",
            "Iteration 181, loss = 0.00688177\n",
            "Iteration 182, loss = 0.00679044\n",
            "Iteration 183, loss = 0.00641952\n",
            "Iteration 184, loss = 0.00656127\n",
            "Iteration 185, loss = 0.00638807\n",
            "Iteration 186, loss = 0.00680661\n",
            "Iteration 187, loss = 0.00756690\n",
            "Iteration 188, loss = 0.00694561\n",
            "Iteration 189, loss = 0.00624804\n",
            "Iteration 190, loss = 0.00616809\n",
            "Iteration 191, loss = 0.00594403\n",
            "Iteration 192, loss = 0.00571114\n",
            "Iteration 193, loss = 0.00566491\n",
            "Iteration 194, loss = 0.00570109\n",
            "Iteration 195, loss = 0.00556780\n",
            "Iteration 196, loss = 0.00529076\n",
            "Iteration 197, loss = 0.00540706\n",
            "Iteration 198, loss = 0.00531420\n",
            "Iteration 199, loss = 0.00563504\n",
            "Iteration 200, loss = 0.00506570\n",
            "Iteration 201, loss = 0.00516465\n",
            "Iteration 202, loss = 0.00486842\n",
            "Iteration 203, loss = 0.00531916\n",
            "Iteration 204, loss = 0.00468949\n",
            "Iteration 205, loss = 0.00528631\n",
            "Iteration 206, loss = 0.00486885\n",
            "Iteration 207, loss = 0.00461745\n",
            "Iteration 208, loss = 0.00519739\n",
            "Iteration 209, loss = 0.00448557\n",
            "Iteration 210, loss = 0.00473346\n",
            "Iteration 211, loss = 0.00479393\n",
            "Iteration 212, loss = 0.00425902\n",
            "Iteration 213, loss = 0.00514692\n",
            "Iteration 214, loss = 0.00449723\n",
            "Iteration 215, loss = 0.00444917\n",
            "Iteration 216, loss = 0.00449520\n",
            "Iteration 217, loss = 0.00444180\n",
            "Iteration 218, loss = 0.00422557\n",
            "Iteration 219, loss = 0.00400401\n",
            "Iteration 220, loss = 0.00398370\n",
            "Iteration 221, loss = 0.00386952\n",
            "Iteration 222, loss = 0.00387257\n",
            "Iteration 223, loss = 0.00376646\n",
            "Iteration 224, loss = 0.00390403\n",
            "Iteration 225, loss = 0.00363327\n",
            "Iteration 226, loss = 0.00362019\n",
            "Iteration 227, loss = 0.00358447\n",
            "Iteration 228, loss = 0.00357043\n",
            "Iteration 229, loss = 0.00345414\n",
            "Iteration 230, loss = 0.00355953\n",
            "Iteration 231, loss = 0.00346573\n",
            "Iteration 232, loss = 0.00342883\n",
            "Iteration 233, loss = 0.00388477\n",
            "Iteration 234, loss = 0.00342439\n",
            "Iteration 235, loss = 0.00336164\n",
            "Iteration 236, loss = 0.00333737\n",
            "Iteration 237, loss = 0.00314361\n",
            "Iteration 238, loss = 0.00324135\n",
            "Iteration 239, loss = 0.00317595\n",
            "Iteration 240, loss = 0.00315089\n",
            "Iteration 241, loss = 0.00323704\n",
            "Iteration 242, loss = 0.00310686\n",
            "Iteration 243, loss = 0.00303888\n",
            "Iteration 244, loss = 0.00292718\n",
            "Iteration 245, loss = 0.00291805\n",
            "Iteration 246, loss = 0.00292768\n",
            "Iteration 247, loss = 0.00297512\n",
            "Iteration 248, loss = 0.00286850\n",
            "Iteration 249, loss = 0.00282626\n",
            "Iteration 250, loss = 0.00307238\n",
            "Iteration 251, loss = 0.00297598\n",
            "Iteration 252, loss = 0.00288551\n",
            "Iteration 253, loss = 0.00274918\n",
            "Iteration 254, loss = 0.00281677\n",
            "Iteration 255, loss = 0.00283454\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.88749887\n",
            "Iteration 2, loss = 0.55932877\n",
            "Iteration 3, loss = 0.39834368\n",
            "Iteration 4, loss = 0.30079042\n",
            "Iteration 5, loss = 0.23715358\n",
            "Iteration 6, loss = 0.20296856\n",
            "Iteration 7, loss = 0.18259122\n",
            "Iteration 8, loss = 0.16969881\n",
            "Iteration 9, loss = 0.15584059\n",
            "Iteration 10, loss = 0.14731541\n",
            "Iteration 1, loss = 1.21838754\n",
            "Iteration 2, loss = 1.13229535\n",
            "Iteration 3, loss = 1.06046550\n",
            "Iteration 4, loss = 0.99759773\n",
            "Iteration 5, loss = 0.93635745\n",
            "Iteration 6, loss = 0.88044038\n",
            "Iteration 7, loss = 0.82363969\n",
            "Iteration 8, loss = 0.76761699\n",
            "Iteration 9, loss = 0.70362336\n",
            "Iteration 10, loss = 0.64249727\n",
            "Iteration 11, loss = 0.58140066\n",
            "Iteration 12, loss = 0.53072462\n",
            "Iteration 13, loss = 0.48408828\n",
            "Iteration 14, loss = 0.44436065\n",
            "Iteration 15, loss = 0.41116275\n",
            "Iteration 16, loss = 0.38140798\n",
            "Iteration 17, loss = 0.35591078\n",
            "Iteration 18, loss = 0.32986976\n",
            "Iteration 19, loss = 0.31043670\n",
            "Iteration 20, loss = 0.29140539\n",
            "Iteration 21, loss = 0.27545871\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 22, loss = 0.26037346\n",
            "Iteration 23, loss = 0.24898467\n",
            "Iteration 24, loss = 0.23808909\n",
            "Iteration 25, loss = 0.22897362\n",
            "Iteration 26, loss = 0.22068036\n",
            "Iteration 27, loss = 0.21344794\n",
            "Iteration 28, loss = 0.20814252\n",
            "Iteration 29, loss = 0.20262223\n",
            "Iteration 30, loss = 0.19745133\n",
            "Iteration 31, loss = 0.19281328\n",
            "Iteration 32, loss = 0.18899713\n",
            "Iteration 33, loss = 0.18590844\n",
            "Iteration 34, loss = 0.18288991\n",
            "Iteration 35, loss = 0.18182224\n",
            "Iteration 36, loss = 0.17730718\n",
            "Iteration 37, loss = 0.17526103\n",
            "Iteration 38, loss = 0.17190881\n",
            "Iteration 39, loss = 0.17034425\n",
            "Iteration 40, loss = 0.16766105\n",
            "Iteration 41, loss = 0.16545888\n",
            "Iteration 42, loss = 0.16430820\n",
            "Iteration 43, loss = 0.16181859\n",
            "Iteration 44, loss = 0.15969846\n",
            "Iteration 45, loss = 0.15826781\n",
            "Iteration 46, loss = 0.15695637\n",
            "Iteration 47, loss = 0.15398530\n",
            "Iteration 48, loss = 0.15222445\n",
            "Iteration 49, loss = 0.15082406\n",
            "Iteration 50, loss = 0.14968921\n",
            "(model1) Accuracy score: 0.9285714285714286\n",
            "(model2) Accuracy score: 0.9047619047619048\n",
            "(model3) Accuracy score: 0.8809523809523809\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "\n",
        "names = ['area', 'perimeter', 'compactness', 'lKer', 'wKer', 'asymKer', 'lKerGroove']\n",
        "lbl = ['Kama', 'Rosa', 'Canadian']\n",
        "classes_dict = dict(zip([x for x in range(len(lbl))], lbl))\n",
        "data = []\n",
        "with open('sample_data/seeds_dataset.txt', 'r') as f:\n",
        "  for line in f:\n",
        "    values = line.strip().split()\n",
        "    data.append([float(v) for v in values])\n",
        "\n",
        "data = np.array(data)\n",
        "print(f'Data: {data}')\n",
        "\n",
        "X = data[:, 0:7]\n",
        "y = data[:, 7]\n",
        "print(X)\n",
        "print(y)\n",
        "\n",
        "# Standardize features (recommended for neural networks)\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
        "\n",
        "# Define MLPClassifier to match Keras model\n",
        "model = MLPClassifier(\n",
        "    hidden_layer_sizes=(32, 16, 4),  # Three hidden layers: 1000, 500, 300 units\n",
        "    activation='relu',                    # ReLU activation\n",
        "    solver='adam',                       # Adam optimizer\n",
        "    max_iter=500,                        # Equivalent to EPOCHS\n",
        "    batch_size=16,                       # Equivalent to BATCH_SIZE\n",
        "    alpha=0.0001,                        # L2 regularization to approximate dropout\n",
        "    random_state=None,                   # No random seed to match Keras\n",
        "    verbose=True,                        # Print training progress\n",
        "    early_stopping=False                 # No validation during training to match Keras\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "pred = model.predict(X_test)  # Direct prediction of class indices\n",
        "as1 = accuracy_score(y_test, pred)\n",
        "\n",
        "\n",
        "model2 = MLPClassifier(\n",
        "    hidden_layer_sizes=(100, 100),  # Three hidden layers: 1000, 500, 300 units\n",
        "    activation='relu',                    # ReLU activation\n",
        "    solver='adam',                       # Adam optimizer\n",
        "    max_iter=10,                        # Equivalent to EPOCHS\n",
        "    batch_size=16,                       # Equivalent to BATCH_SIZE\n",
        "    alpha=0.0001,                        # L2 regularization to approximate dropout\n",
        "    random_state=None,                   # No random seed to match Keras\n",
        "    verbose=True,                        # Print training progress\n",
        "    early_stopping=False                 # No validation during training to match Keras\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "model2.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "pred2 = model2.predict(X_test)  # Direct prediction of class indices\n",
        "as2 = accuracy_score(y_test, pred2)\n",
        "\n",
        "model3 = MLPClassifier(\n",
        "    hidden_layer_sizes=(16, 16),  # Three hidden layers: 1000, 500, 300 units\n",
        "    activation='relu',                    # ReLU activation\n",
        "    solver='adam',                       # Adam optimizer\n",
        "    max_iter=50,                        # Equivalent to EPOCHS\n",
        "    batch_size=16,                       # Equivalent to BATCH_SIZE\n",
        "    alpha=0.0001,                        # L2 regularization to approximate dropout\n",
        "    random_state=None,                   # No random seed to match Keras\n",
        "    verbose=True,                        # Print training progress\n",
        "    early_stopping=False                 # No validation during training to match Keras\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "model3.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "pred3 = model3.predict(X_test)  # Direct prediction of class indices\n",
        "as3 = accuracy_score(y_test, pred3)\n",
        "\n",
        "# Print accuracy score\n",
        "print(f'(model1) Accuracy score: {as1}')\n",
        "print(f'(model2) Accuracy score: {as2}')\n",
        "print(f'(model3) Accuracy score: {as3}')\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}