# -*- coding: utf-8 -*-
"""ML_Lab10Ex1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vpQxd1kFo8Fgiax0uZP4qPrSWUX0rpbe
"""

import numpy as np
from sklearn.datasets import load_diabetes
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Load and preprocess data
diabetes = load_diabetes(as_frame=True)
print(diabetes.DESCR)

df = diabetes.frame
X = diabetes.data
y = diabetes.target

print(f'df: {df.shape}')
print(f'X: {X.shape}')
print(f'y: {y.shape}')

# First split: 80% and 20% (test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)

# Define MLPRegressor to match Keras model
model = MLPRegressor(
    hidden_layer_sizes=(64, 64),  # Two hidden layers with 64 units each
    activation='relu',            # ReLU activation
    solver='adam',               # Adam optimizer
    max_iter=500,                # Equivalent to EPOCHS
    batch_size=16,               # Equivalent to BATCH_SIZE
    random_state=42,             # For reproducibility
    verbose=True,                # Print training progress
    early_stopping=True,         # Enable early stopping
    validation_fraction=0.1,     # Use 10% of training data for internal validation
    n_iter_no_change=10,         # Stop if no improvement for 10 iterations
    tol=1e-4                     # Tolerance for convergence
)

# Train the model
model.fit(X_train, y_train)

# Test metrics (using X_test, y_test)
y_test_pred1 = model.predict(X_test)
mse1 = mean_squared_error(y_test, y_test_pred1)
r21 = r2_score(y_test, y_test_pred1)

model2 = MLPRegressor(
    hidden_layer_sizes=(100, 100),  # Two hidden layers with 64 units each
    activation='relu',            # ReLU activation
    solver='adam',               # Adam optimizer
    max_iter=1000,                # Equivalent to EPOCHS
    batch_size=32,               # Equivalent to BATCH_SIZE
    random_state=42,             # For reproducibility
    verbose=True,                # Print training progress
    early_stopping=True,         # Enable early stopping
    validation_fraction=0.1,     # Use 10% of training data for internal validation
    n_iter_no_change=10,         # Stop if no improvement for 10 iterations
    tol=1e-4                     # Tolerance for convergence
)

# Train the model2
model2.fit(X_train, y_train)

# Test metrics (using X_test, y_test)
y_test_pred2 = model2.predict(X_test)
mse2 = mean_squared_error(y_test, y_test_pred2)
r22 = r2_score(y_test, y_test_pred2)

model3 = MLPRegressor(
    hidden_layer_sizes=(32, 32),  # Two hidden layers with 64 units each
    activation='relu',            # ReLU activation
    solver='adam',               # Adam optimizer
    max_iter=100,                # Equivalent to EPOCHS
    batch_size=16,               # Equivalent to BATCH_SIZE
    random_state=42,             # For reproducibility
    verbose=True,                # Print training progress
    early_stopping=True,         # Enable early stopping
    validation_fraction=0.1,     # Use 10% of training data for internal validation
    n_iter_no_change=10,         # Stop if no improvement for 10 iterations
    tol=1e-4                     # Tolerance for convergence
)

# Train the model2
model3.fit(X_train, y_train)

# Test metrics (using X_test, y_test)
y_test_pred3 = model3.predict(X_test)
mse3 = mean_squared_error(y_test, y_test_pred3)
r23 = r2_score(y_test, y_test_pred3)
print(f"(Model1) R2 Accuracy Score: {r21:.4f}")
print(f"(Model1) MSE: {mse1:.4f}")
print(f"(Model2) R2 Accuracy Score: {r22:.4f}")
print(f"(Model2) MSE: {mse2:.4f}")
print(f"(Model3) R2 Accuracy Score: {r23:.4f}")
print(f"(Model3) MSE: {mse3:.4f}")

